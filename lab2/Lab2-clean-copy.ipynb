{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93812d4f",
   "metadata": {},
   "source": [
    "ECE479ICC LAB 2: Raspberry Pi, TF-Keras, and Face Detection\n",
    "====================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac27d92",
   "metadata": {},
   "source": [
    "## PART I: Raspberry Pi Setup and Basics\n",
    "\n",
    "### Raspberry Pi Setup\n",
    "\n",
    "From this lab on, you will apply what you have learned in the lectures to train and deploy a neural network on an edge device, Raspberry Pi 4.\n",
    "\n",
    "Raspberry Pi 4 is an inexpensive but powerful IoT device. You may review this [report][raspberry_pi_history] to learn more about its story. To start, you need to install the proper operating system, you can refer to this [tutorial][headless_rpi_start]. You can use the official [imager] to prepare your OS, you should use this [image_file][image_link]. If you are at home without a monitor, you can set up the OS with the headless option. \n",
    "\n",
    "Please notice that we are using **Raspberry Pi 4**, with **custom OS(provided)**.\n",
    "\n",
    "Once your RPi can boot into the OS normally, you can enable the VNC and access the desktop environment using your laptop/desktop. You can follow the \"Enabling and Connecting over VNC\" section of the [tutorial][headless_rpi_start] for this.\n",
    "\n",
    "Now that you have the desktop environment, follow this [guide][picam_start] to set up your pi-camera. You should be able to take pictures and videos with your camera.\n",
    "\n",
    "[raspberry_pi_history]:https://www.techrepublic.com/article/inside-the-raspberry-pi-the-story-of-the-35-computer-that-changed-the-world/\n",
    "[image_link]:https://github.com/google-coral/aiy-maker-kit-tools/releases/download/v20220518/aiy-maker-kit-2022-05-18.img.xz\n",
    "[imager]:https://www.raspberrypi.com/software/\n",
    "[headless_rpi_start]: https://www.tomshardware.com/reviews/raspberry-pi-headless-setup-how-to,6028.html\n",
    "[rpi_headless]:https://www.raspberrypi.com/documentation/computers/configuration.html#setting-up-a-headless-raspberry-pi\n",
    "[rpi_ssh]:https://www.raspberrypi.com/documentation/computers/remote-access.html\n",
    "[picam_start]:https://picamera.readthedocs.io/en/latest/\n",
    "[headless_setup]:https://www.raspberrypi.org/documentation/remote-access/ssh/\n",
    "\n",
    "In this part of the lab, you need to complete the following tasks: \n",
    "\n",
    "  * Install Raspbian OS in your Raspberry Pi, set up your network connection, and take a selfie on the pi-camera.\n",
    "  * **Print** the CPU specs with `cat /proc/cpuinfo`\n",
    "  * **Print** network interface configuration with `ifconfig`\n",
    "  \n",
    "  \n",
    "### TensorFlow Setup on Raspberry Pi \n",
    "\n",
    "Once you can log into the Raspberry Pi desktop, please verify to make sure tf_lite is correctly installed in the raspberry pi. \n",
    "By running\n",
    "```Python\n",
    "import tflite_runtime.interpreter as tflite\n",
    "```\n",
    "in a python3 environment. If there is no error, the installation is done.\n",
    "\n",
    "If error occurs, you can follow the [this_guide][tf_lite_install] and [this_guide][coral_setup] to install **TensorFlow Lite** on your Raspberry Pi and work with the accelerator.\n",
    "\n",
    "[tf_lite_install]:https://www.tensorflow.org/lite/guide/python\n",
    "[coral_setup]:https://coral.ai/docs/accelerator/get-started/\n",
    "\n",
    "### TensorFlow Setup on Your PC\n",
    "\n",
    "Please follow the following commands in your conda environment to install **TensorFlow 2.10** on your own PC.\n",
    "```\n",
    "conda install -c conda-forge tensorflow=2.10\n",
    "```\n",
    "If you have a Apple Silicon Mac, please follow this [guide][tf_install]. It is a little more complicated.\n",
    "\n",
    "\n",
    "After the installation. Please verify that the installation is properly done. By running\n",
    "```Python\n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "```\n",
    "in a python environment, notice the version number for your Tensorflow.\n",
    "\n",
    "[tf_install]:https://towardsdatascience.com/installing-tensorflow-on-the-m1-mac-410bb36b776"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbce25d",
   "metadata": {},
   "source": [
    "## PART II: Building and Training with TF-Keras\n",
    "\n",
    "In part II, you will build a Convolutional Neural Network (CNN) based image classifier. In our lecture, we have introduced CNN, a type of Deep Neural Network (DNN) that features convolutional layers. This [document][cnn_definition] by Stanford CS231n is also a good resource to learn more about CNN. You will build and train a simple CNN with TensorFlow on your own computers.\n",
    "\n",
    "[cnn_definition]:http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "There were two ways to construct and train a neural network model in TF1.x: Keras and low-level APIs. As Tensorflow evolves to 2.x, it leans more toward Keras, a higher-level framework than the previous version's low-level APIs. Keras APIs provide straightforward construction and training of neural networks, which hide away some tedious details of the network. \n",
    "\n",
    "Some may argue that low-level APIs provide more precise control over some aspects of the network. While users can still find a way to tune the details by looking deeper into the APIs, the framework encourages them to focus on the big picture and develop better and more robust network models rather than fixating on trivialities. As a beginner, it is helpful for us to start with Keras and establish a basic intuition on how to build and train a simple network.\n",
    "\n",
    "In this part of the lab, you need to follow this [tutorial][fashion_mnist] to build a neural network to classify the Fashion MNIST dataset.\n",
    "\n",
    "[fashion_mnist]:https://www.tensorflow.org/tutorials/keras/classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fea53b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### FashionNet\n",
    "\n",
    "You will construct a neural network by yourselves. Let's call it the FashionNet. Here is an overview of the proposed Convolutional Neurual Network (CNN):\n",
    "\n",
    "![cnn_img](figures/fashionnet.png)\n",
    "\n",
    "This CNN contains two convolutional layers, two max pooling layers, and three fully connected layers (dense layers). The configuration details of each layer are shown in the following table (in sequential order):\n",
    "\n",
    "| Layers          | Configuration                        | Activation | Output Dimensions |\n",
    "|---------------:|--------------------------------------:|-------------------:|----------------------------------------: |\n",
    "| convolution     | input size = (28, 28, 1), kernel size = (5, 5), stride = 1, padding = 'same'   |       ReLU          |            (28, 28, 4)                     |\n",
    "| max pooling     | pool_size = (2, 2), stride = 2                           |       -             |            (14, 14, 4)                     |\n",
    "| convolution     | kernel size = (3, 3), stride = 1, padding = 'valid'|       ReLU          |            (12, 12, 8)                     |\n",
    "| max pooling     | pool_size = (2, 2), stride = 2                           |       -             |            (6, 6, 8)                       |\n",
    "| dense |       |       ReLU          |            (128)                     |\n",
    "| dense |        |       ReLU          |            (64)                      |\n",
    "| dense |         |       Softmax       |            (10)                      |\n",
    "\n",
    "\n",
    "### Dataset\n",
    "In this lab, you will use the Fashion MNIST dataset. You can load the dataset in the following way, which is also shown in the tutorial:\n",
    "```Python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "```\n",
    "As discussed in the lecture, you should only use `train_images` and `train_labels` to train your model. You may also want to validate your model with a training/validation split. We will discuss the details in the next section.\n",
    "\n",
    "### Build, Train, and Evaluate\n",
    "Generally, there are three stages in the process of developing a neural network: build, train, and evaluate. In Keras, there are corresponding functions for each stage:\n",
    "\n",
    "1. **Build** First, you define the model architecture. In this lab, you can use a simple `keras.Sequential` function to define a model, as shown in this [example][sequential]. There are other ways to define a model, but we will not discuss those in this lab. After you have defined your model, you need to tell Keras what you want to do with it by compiling it. In the `model.compile` function, you specify the optimizer, loss function, metrics, and other configurations to prepare for training.\n",
    "2. **Train** You use `model.fit` function to train your model. In this function, you can specify the training parameters, including batch size, epochs, etc. You should specify your validation split in this function too. Please refer to the document and see how to do that.\n",
    "3. **Evaluate** After you have trained your model, you need to test the final accuracy using the testing data. The `model.evaluate` function gives you the percentage of the correct results. \n",
    "\n",
    "You can find the document for the above functions [here][fit_api].\n",
    "You may also find that in FashionNet, you use 2-D convolution in the first couple of layers. You may find [this document][layer_api] helpful since the `layers.Conv2D` and `layers.MaxPool2D` functions are not covered in the tutorial.\n",
    "\n",
    "The training of this CNN should not take a very long time on a laptop/desktop CPU. The final testing accuracy should be higher than 85%, and the training accuracy should be higher than 90%.\n",
    "\n",
    "\n",
    "### Plot the Training Progress\n",
    "The `model.fit` function outputs a history of the training metrics (accuracy and loss), including validation metrics. This infomation can be very helpful for diagnosing the model.\n",
    "\n",
    "**Use `matplotlib` to graph the model accuracy and loss on the training and validation dataset, and include them in your report.** The graph should look something like the following, but yours can be different if you have different configurations of the training process.\n",
    "\n",
    "![train_img](figures/train.png)\n",
    "\n",
    "### Saving Your Trained Model\n",
    "Finally, you will need to save your trained models to model files. Saving a trained model allows you to deploy it to other devices or restore previous training progress at a later time. \n",
    "\n",
    "For the purpose of this lab, we ask you to save it using **the TensorFlow SavedModel format**, which will contain the information of the model as well as the weights.\n",
    "\n",
    "```Python\n",
    "# Calling `save('my_model')` creates a SavedModel folder `my_model`.\n",
    "your_trained_model.save(\"my_model\")\n",
    "```\n",
    "\n",
    "Please read [this document][save_model] to learn more about how the model save works in tensorflow.\n",
    "\n",
    "[build_tag]:https://www.tensorflow.org/tutorials/keras/classification#build_the_model\n",
    "[train_tag]:https://www.tensorflow.org/tutorials/keras/classification#train_the_model\n",
    "[eval_tag]:https://www.tensorflow.org/tutorials/keras/classification#evaluate_accuracy\n",
    "[sequential]:https://www.tensorflow.org/guide/keras/sequential_model#when_to_use_a_sequential_model\n",
    "\n",
    "\n",
    "[fit_api]:https://keras.io/api/models/model_training_apis/\n",
    "[layer_api]:https://www.tensorflow.org/api_docs/python/tf/keras/layers\n",
    "[save_model]:https://www.tensorflow.org/guide/keras/save_and_serialize#what_the_savedmodel_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af6b96fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0-rc0\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b973657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO:\n",
    "# 1. Load Fashion MNIST dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "train_labels\n",
    "\n",
    "print(set(train_labels))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(train_images[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a260b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build, Train, and Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73acd591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728f912a",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Convert Your Trained Model to TFLite model\n",
    "\n",
    "Before you start the migration process of your trained model onto our edge devices, Raspberry Pi 4, please make sure you have followed the previous steps carefully and have a properly trained TF model with reasonable accuracy on the validation dataset.\n",
    "\n",
    "Now, locate the directory where your trained model files are saved and create a TFLite converter ready for the next step. \n",
    "\n",
    "Please refer to [this document][tf_lite_converter] for more information on the TFLite model.\n",
    "\n",
    "[tf_lite_converter]:https://www.tensorflow.org/lite/api_docs/python/tf/lite/TFLiteConverter#from_saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d595b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert to TFLite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8171c68b",
   "metadata": {},
   "source": [
    "### Optimize your TFLite model with post-trainning quantization\n",
    "With the converter, we can directly transform our model to TFLite model; however, edge and IoT devices are usually low-powered and relatively lack computational power compared to PC. Therefore, to further enhance the performance, you can apply the post-training quantization technique before deploying the model onto the Raspberry Pi.\n",
    "\n",
    "Notice that, although the post-training quantization is similar to the quantization and retrain technique that we will introduce in Lecture 12, since the model will not be retrained after the quantization, the impact on the accuracy of the model is usually larger.\n",
    "\n",
    "In this part, you will need to apply two quantization methods, namely, **Dynamic range quantization** and **Full integer quantization**, listed in [this document][post-trainning quantization] to your pre-trained models and compare the accuracy and performance.\n",
    "\n",
    "1) For dynamic range quantization, quantization parameters are automatically detected and generated by TensorFlow. Please write a piece of code to convert your original trained model into a TFLite model using dynamic range quantization.  \n",
    "\n",
    "2) For integer quantization, TensorFlow needs to sample the dataset you used for training the models to estimate the **MIN** and **MAX** values. This process can be described as \"representative data generation\". An example representative data generator is\n",
    "```Python\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(trainning_dataset).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "```\n",
    "\n",
    "You will need to define your own representative data generator function and properly select the following parameters as below:\n",
    "```Python\n",
    "# dataset used for training\n",
    "trainning_set\n",
    "# batch number x\n",
    ".batch(x)\n",
    "# input y\n",
    ".take(y)\n",
    "```\n",
    "\n",
    "Please refer to this [official document][from_tensor_slices] for explanations on the **from_tensor_slices()** API. On the same page, you will also learn about how the TensorFlow ***tf.Data.Dataset*** format works.\n",
    "\n",
    "[from_tensor_slices]:https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices\n",
    "[post-trainning quantization]:https://www.tensorflow.org/lite/performance/post_training_quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21fb5d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. Dynamic range quantization\n",
    "# 2. Full integer quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e2050",
   "metadata": {},
   "source": [
    "### Save and deploy your TFLite model\n",
    "Save your converted TFLite models as binary files. Example code:\n",
    "```Python\n",
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)\n",
    "```\n",
    "\n",
    "To run execute the TFLite model on the RPi, you can refer to [this example][TFLite] for the details. In general, the following code is the standard process.\n",
    "#### Step 1: load the model and allocate the tensors\n",
    "We first need to instantiate the model with the TFLite model file, and then allocate all the tensors.\n",
    "```Python\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "```\n",
    "#### Step 2: Check input and output info\n",
    "To get the input and output tensor's names and their corresponding sizes, we will first need do the following steps to extract these information.\n",
    "```Python\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "```\n",
    "#### Step 3: Set input tensor and run the model\n",
    "Once we got the input tensor's name and shape, we can then set the input tensor and run the model.\n",
    "```Python\n",
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run the model\n",
    "interpreter.invoke()\n",
    "```\n",
    "#### Step 4: Extract the output tensor\n",
    "We can finally run the `get_tensor(...)` function again to extract the output.\n",
    "```Python\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)\n",
    "```\n",
    "Once you confirm that the model is deployable on your RPi, you are ready to use the Coral Edge TPU to accelerate your computing. Please follow this [tutorial][install] to install required Edge TPU runtime. Follow [tutorial][TPU] to run your converted TFLite model on your Raspberry Pi with the Edge TPU.\n",
    "\n",
    "**Please convert your model with these two different methods and report their corresponding accuracy and inference time with the TFLite model on the RPi with and without the Edge TPU. Compare and comment on what you observed in the report.**\n",
    "\n",
    "[install]:https://coral.ai/docs/accelerator/get-started/#1-install-the-edge-tpu-runtime\n",
    "[TFLite]:https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python\n",
    "[TPU]:https://coral.ai/docs/accelerator/get-started/#3-run-a-model-on-the-edge-tpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572578a",
   "metadata": {},
   "source": [
    "## PART III: Face Detection and Recognition\n",
    "\n",
    "In part III, you will have a chance to utilize your Raspberry Pi 4 as a powerful edge platform for human face detection and recognition applications. You will need to capture valid images using your PiCamera and port them into the recognition neural network with proper processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c76878e",
   "metadata": {},
   "source": [
    "### Prepare a Pre-trained Convolutional Neural Network\n",
    "#### **Introduction** \n",
    "\n",
    "In this part, you will use the award-winning \"very deep neural network\" for face recognition. It is the ILSVRC 2014 winner, GoogLeNet, and it also has the name ***Inception ResNet v1***. For more information, please refer to this [review][review_googlenet], and the original paper could be found [here][googlenet_paper]. The goal of this part is to reconstruct the Inception ResNet v1 with Keras APIs, load the given pre-trained weights, and convert the whole model into a TFLite model. You will need the knowledge from the previous parts of the lab.\n",
    "\n",
    "![googleNet](figures/overview.png)\n",
    "\n",
    "The original GooLeNet (Inception ResNet v1) is huge and complex. It has 22 major layers and more than 10 milion trainable parameters. Thus, it will be painful to reconstruct it using the same way you learned in **Part II** of this lab, i.e., listing out all layers in order by using `keras.Sequential` function.\n",
    "\n",
    "There is a better way to define a model architecture. You can use the pre-built Keras APIs to reconstruct the Inception Resnet v1. You can find all the layers [here][tf_keras_modules]. It may be helpful to have this page opened on aside while working on this part.\n",
    "\n",
    "For example, to apply two `tf.keras.layers.Conv2D`layers to the existing model `x`, you just need to \n",
    "```Python\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "x = Conv2D(...)(x)\n",
    "x = Conv2D(...)(x)\n",
    "```\n",
    "The output of model `x` on the left side will be connected by a new `Conv2D` layer and the resulting new model will be assigned to `x` on the right side. By doing this repeatedly in proper order, you achieve the same goal as using `keras.Sequential`.\n",
    "\n",
    "The given files structure is listed as,\n",
    "```bash\n",
    ".\n",
    "├── lab2_part3.py\n",
    "├── inception_resnet.py\n",
    "├── resnet_block.py\n",
    "├── conv2d_bn.py\n",
    "├── modules.py \n",
    "└── weights\n",
    "    └── inception_keras_weights.h5\n",
    "```\n",
    "\n",
    "We have provided the `modules.py` and `resnet_block.py` for you, and we expect you NOT to modify this file. You will need to implement code blocks within `conv2d_bn.py` and `inception_resnet.py` according to the instructions and parameters given to you. \n",
    "\n",
    "Because of the dependencies between functions, it's highly recommended that you implement the codes following the order of steps below. **Notice: while applying a new Keras layer, please make sure you include the `name=` argument.** This will not affect the functionality of your model, but it's helpful for you to locate problematic layers. You can follow the convention of the example blocks/layers to name your layers/blocks with the `name_fmt` and `generate_layer_name` function.\n",
    "\n",
    "[review_googlenet]:https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7\n",
    "[googlenet_paper]:https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf\n",
    "[tf_keras_modules]:https://www.tensorflow.org/api_docs/python/tf/keras/layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e53328",
   "metadata": {},
   "source": [
    "#### Step 1: **Implement `conv2d_bn(*)` function**\n",
    "\n",
    "`conv2d_bn(*)` is defined in the `conv2d_bn.py`. We have imported all the Keras layer APIs for you, and thus you can directly use them by calling their names, such as [`Conv2D`][Conv2D] and [`Activation`][Activation].\n",
    "\n",
    "This function is relatively simple and helps you get familiar with the Keras format. Look for **`## TO DO`** in the function.\n",
    "\n",
    "`## TO DO Step1`: Apply a `Conv2D` layer to model `x`, with all useful parameters listed as arguments in the function signature of`conv2d_bn(*)`. Assign the new model back to `x`. When calling `Conv2D`, make sure all the available input arguments for the `conv2d_bn()` function are used.\n",
    "\n",
    "`## TO DO Step2`: Apply a Batch Normalization layer to model `x`, with \n",
    "\n",
    "| Arguments        | Values                        \n",
    "|---------------:|--------------------------------------:|\n",
    "| axis |             bn_axis          |          \n",
    "| momentum |              0.995          |          \n",
    "| epsilon |               0.001       |           \n",
    "| scale |               False       |    \n",
    "| name  |    bn_name  |\n",
    "\n",
    "`## TO DO Step3`: Apply an Activation layer to model `x`, with the argument `activation` passed into the `conv2d_bn(*)`. Use `ac_name` as the name for the layer.\n",
    "\n",
    "[Conv2D]:https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\n",
    "[Activation]:https://www.tensorflow.org/api_docs/python/tf/keras/layers/Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe8179",
   "metadata": {},
   "source": [
    "#### Step 2: **Familiarize with `inception_resnet_block(*)` function**\n",
    "\n",
    "`inception_resnet_block(*)` is defined in the `resnet_block.py`. The function defines three different types of Inception ResNet blocks `Inception_block_a`, `Inception_block_b`and `Inception_block_c`.\n",
    "\n",
    "The function implements reusable building blocks for inception blocks in the Inception ResNet v1. You can imagine this function as the pre-built bricks in LEGO, and what you need to do is to put them in the proper places when you build the entire network.\n",
    "\n",
    "[ResNet][resnet_ex] or Residual Netoworks has a very special stucture called **shortcut**. \n",
    "\n",
    "<div>\n",
    "<img src=\"figures/resnet_shortcut.png\" width=\"200\">\n",
    "</div>\n",
    "\n",
    "\n",
    "In a more general sense, it allows multiple parallel paths, or \"branches\", through the neural network. These \"branches\" are merged or aggregated by a specific layer named `Concatenate`. For example, in `Inception_block_c`, we have two branches: `branch_0` and `branch_1`. `branch_0` has one Conv 2D layer with 192 output channels and a kernel size of *1x1*. `branch_1` has three Conv 2D layers with 192 output channels and kernel sizes of *1x1*, *1x3*, and *3x1*.\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/inception_c.png\" width=\"200\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "[resnet_ex]:https://d2l.ai/chapter_convolutional-modern/resnet.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e2fc3e",
   "metadata": {},
   "source": [
    "#### Step 3: **Finish the Inception ResNet Structure**\n",
    "\n",
    "Now, with the building blocks that you have prepared in previous steps, you have three powerful tools to build the actual Inception ResNet v1, a very deep convolutional neural network that has 448 layers and sublayers. (You do not even want to list out all of these layers.)\n",
    "\n",
    "The overview of the Inception ResNet v1 is,\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/inception_resnet_overview.png\">\n",
    "</div>\n",
    "\n",
    "\n",
    "You need `tensorflow.keras.layers`, `conv2d_bn(*)`, and `resnet_block(*)` to build the network. Follow the instructions and hints below to finish the whole convolutional neural network.\n",
    "\n",
    "`## TO DO Step1`: Finish the Maxpooling 2D preprocessing for model `x` with,\n",
    "\n",
    "| Layers          | Configuration                        |Output Dimensions |\n",
    "|---------------:|--------------------------------------:| ----------------------------------------: |\n",
    "| conv2d_bn |  input size = (79, 79, 32), kernel size = (3, 3), padding='valid' | (77, 77, 32) |\n",
    "| conv2d_bn |  connect to previous, kernel size = (3, 3) | (77, 77, 64) |     \n",
    "| MaxPooling2D |  connect to previous, pool_size = (2, 2), stride = 2   | (38, 38, 64) |\n",
    "| conv2d_bn |  connect to previous, kernel size = (1, 1), padding='valid' | (38, 38, 80) |\n",
    "| conv2d_bn |  connect to previous, kernel size = (3, 3), padding='valid'  | (36, 36, 192) |\n",
    "| conv2d_bn |  connect to previous, kernel size = (3, 3), stride = 2, padding='valid'  | (17, 17, 256) |\n",
    "  \n",
    "\n",
    "`## TO DO Step2`: Instantiate **5 connected** Inception ResNet *block_type_a* using,\n",
    "\n",
    "| scale         | block_idx       | block_type |\n",
    "|---------------:|------------:| --------------------------: |\n",
    "| 0.17 |  1 to 5 | Inception_block_a |\n",
    "\n",
    "`## TO DO Step3`: Instantiate **10 connected** Inception ResNet *block_type_b* using,\n",
    "\n",
    "| scale         | block_idx         | block_type |\n",
    "|---------------:|------------------:| -------------------------: |\n",
    "| 0.1 |  1 to 10 | Inception_block_b |\n",
    "\n",
    "`## TO DO Step4`: Instantiate **5 connected** Inception ResNet *block_type_c* using,\n",
    "\n",
    "| scale         | block_idx       | block_type |\n",
    "|---------------:|----------------:| -----------------: |\n",
    "| 0.2 |  1 to 5 | Inception_block_c |\n",
    "\n",
    "`## TO DO Step5`: Apply Global Average Pooling (`GloablAveragePooling2D(...)`) + Dropout (`Drop(...)`) layers to model `x`. You can find the documentation of these layers [here][GAP] and [here][Dropout]. For the Dropout layer, use `dropout_keep_prob` to properly calculate the `rate` argument of the `Dropout` layer. (Hint: what's the relationship between \"keep\" and \"dropout\"?)\n",
    "\n",
    "[GAP]:https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D\n",
    "[DropOut]:https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e823952",
   "metadata": {},
   "source": [
    "####  **Step 4: Compile the model and load weights**\n",
    "\n",
    "The structure of Inception ResNet v1 is completed and ready to be compiled. Implement your code in `lab2_part3.py`. This file will be the place where your main logic should go.\n",
    "\n",
    "To compile a model, simply do\n",
    "```Python\n",
    "model = InceptionResNetV1Norm();\n",
    "```\n",
    "**You may see some warnings due to TensorFlow version difference, but they do not usually matter.*\n",
    "\n",
    "Now try to print your model's summary to find out the number of trainable parameters and the number of layers. You will need these numbers while writing your report.\n",
    "\n",
    "To load the pre-trained weights, use `model.load_weights(*)`. We have prepared the weights for you located at `\"weights/inception_keras_weights.h5\"`.\n",
    "\n",
    "At this point, if your model loads without any other errors, then your model implementation is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f728410e",
   "metadata": {},
   "source": [
    "#### Step 5: **Convert the model into TFLite model**\n",
    "If you reach here without any problem, your implementation for Inception ResNet v1 should be basically correct. \n",
    "\n",
    "Unfortunately, there is no easy way to partially verify your implementation, so if you meet any errors that stall the process, you should go back to previous steps to fix any existing problem before proceeding.\n",
    "\n",
    "Follow the same instructions as **Convert Your Trained Model to TFLite model** in **Part II**. You **do not** need to apply post-training quantization this time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2235ab28",
   "metadata": {},
   "source": [
    "### Face detection and recognition with MTCNN & FaceNet and integration with PiCamera\n",
    "\n",
    "Now that you have converted our network to a TensorFlow Lite model, you can start integrating all the components that you have implemented so far towards a more realistic face detection and recognition system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd51bd",
   "metadata": {},
   "source": [
    "#### Step 1: Image capturing with PiCamera\n",
    "The first step of the work is to implement a simple function that can capture an image using the PiCamera and convert it to the forms that the neural networks can use. More specifically, in this lab, you will use the very basic image capturing feature of the PiCamera and transform the captured image into an OpenCV object.\n",
    "\n",
    "You can refer to the \"Capturing to an OpenCV object\" section of the [PiCamera package documentation][PiCamera] for the details of capturing images with the PiCamera and fill in the following image function with the given signature. The returned object should be a 3-D tensor, with the three channels in RGB order.\n",
    "\n",
    "[PiCamera]:https://picamera.readthedocs.io/en/release-1.10/recipes1.html#capturing-to-an-opencv-object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "241d7539",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'picamera'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpicamera\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcapture_image\u001b[39m():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Instrctor note: this can be directly taken from the PiCamera documentation\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Create the in-memory stream\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'picamera'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import picamera\n",
    "import numpy as np\n",
    "def capture_image():\n",
    "    # Instrctor note: this can be directly taken from the PiCamera documentation\n",
    "    # Create the in-memory stream\n",
    "    stream = io.BytesIO()\n",
    "    with picamera.PiCamera() as camera:\n",
    "        camera.capture(stream, format='jpeg')\n",
    "        \n",
    "    # Construct a numpy array from the stream\n",
    "    data = np.frombuffer(stream.getvalue(), dtype=np.uint8)\n",
    "    \n",
    "    # \"Decode\" the image from the array, preserving colour\n",
    "    image = cv2.imdecode(data, 1)\n",
    "    \n",
    "    # OpenCV returns an array with data in BGR order. \n",
    "    # The following code invert the order of the last dimension.\n",
    "    image = image[:, :, ::-1]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a542a5a",
   "metadata": {},
   "source": [
    "#### Step 2: Face detection\n",
    "\n",
    "In general, face detection can be seen as a special case of object detection, where there is only one object class, namely, \"human face\". The [Multi-Task Cascaded Convolutional Networks (MTCNN)][MTCNN] is one of the pioneering works in this area. It is purposedly built for detecting human faces and also identifying the keypoints (eyes, nose, and mouth). It is one of the most popular and successful networks in this area.\n",
    "\n",
    "For this part of the lab, you will use an easy-to-use Python package for MTCNN, which provides a simple API for performing face detections, which suites our needs for this lab. However, you should notice that, while having a convenient python package, MTCNN is not dedicated to real-time detection on mobile devices. Therefore the inference latency is a few seconds on the Raspberry Pi.\n",
    "\n",
    "For this step, you will first instantiate the MTCNN from the mtcnn package. Then, write a function that does the following operations. The function prototype is given to you:\n",
    "1. Perform face detection with MTCNN and extract the bounding box of the first image.\n",
    "2. Add a 20% margin to each dimension of the bounding box, such that the bounding box is 120% of the original size in each dimension. That is, the bounding box is expanded by 10% in each direction(up, down, left, right).\n",
    "3. Perform image cropping according to the coordinate of the extended bounding box.\n",
    "4. Return the cropped image and the coordinates (x, y, w, h) of the bounding box.\n",
    "5. Finally, display the bounding box in the original image.\n",
    "\n",
    "To help you implement the code, here are some useful tips and links:\n",
    "- For simplicity, you can assume that there is only one face in the input image.\n",
    "- The basic usage of MTCNN can be found in the MTCNN python package description [here][MTCNN_package]. As you may find out, the interface is very compact and intuitive. For this lab, you only need the 'box' from the output, which specifies the coordinates of a corner and the dimensions of the bounding box.\n",
    "- Performing the cropping on the OpenCV image is basically the same as working on a NumPy array, which you have already implemented in Lab 1. So you can borrow from your Lab 1 code if needed.\n",
    "- To help you to draw the bounding box, we provide a function here that can do it for you.\n",
    "\n",
    "[MTCNN]:https://kpzhang93.github.io/MTCNN_face_detection_alignment/\n",
    "[MTCNN_package]:https://pypi.org/project/mtcnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn.mtcnn import MTCNN\n",
    "def detect_and_crop(mtcnn, image):\n",
    "    detection = mtcnn.detect_faces(image)[0]\n",
    "    #TODO\n",
    "    \n",
    "    #return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f057680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function provided for the students to draw the rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "def show_bounding_box(image, bounding_box):\n",
    "    x1, y1, w, h = bounding_box\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ax.imshow(image)\n",
    "    ax.add_patch(Rectangle((x1, y1), w, h, linewidth=1, edgecolor='r', facecolor='none'))\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db282f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN()\n",
    "image = capture_image()\n",
    "cropped_image = detect_and_crop(mtcnn, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178b9bbb",
   "metadata": {},
   "source": [
    "### Step 3: Face Recognition\n",
    "\n",
    "Once the faces are detected in the picture, you can then use your FaceNet to perform recognition of the faces. This step is a continuation of Part 3.1, and you now need to pass the image to the face recognition network and get the output feature vector.\n",
    "\n",
    "Before passing the input image to the model, you need to preprocess the image, which includes resizing the image to the expected size of the model and standardize values across the channels. For your convenience, the preprocessing function is provided.\n",
    "\n",
    "The run_model function should perform the following tasks:\n",
    "- Extract the input and output shape details from the model\n",
    "- Set the input tensor\n",
    "- Invoke the model\n",
    "- Extract and return the output tensor\n",
    "\n",
    "This is almost a standard process of running TensorFlow Lite models. You can refer to [this example][TFLite] for the details, you need very few changes in general.\n",
    "\n",
    "Finally, write a short script that calls the functions and perform the following steps:\n",
    "1. Load the TensorFlow Lite model\n",
    "2. Allocate the tensors\n",
    "3. Preprocess the cropped image\n",
    "4. Run the model on the preprocessed image and save the output tensor\n",
    "\n",
    "[TFLite]:https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39945df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing function provided to the students\n",
    "def pre_process(face, required_size=(160, 160)):\n",
    "    ret = cv2.resize(face, required_size)\n",
    "    ret = ret.astype('float32')\n",
    "    mean, std = ret.mean(), ret.std()\n",
    "    ret = (ret - mean) / std\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a88a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, face):\n",
    "# students will need to fill in the following function\n",
    "    #TODO\n",
    "    #return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a74cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfl_file = \"./code/model.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=tfl_file)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1873addb",
   "metadata": {},
   "source": [
    "### Step 4: Verification and Report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0cf360",
   "metadata": {},
   "source": [
    "To verify the correct functionality of your face recognition system, you will need to calculate the euclidean distances between the feature vectors of pictures of yourself and other people. If the distance between your pictures is significantly smaller than the distance between the pictures of you and other people, then your network basically functions as expected.\n",
    "\n",
    "The distance between the feature vectors can be evaluated with the euclidean distance as you have implemented in Lab 1. We provide a read_image() function here for you to read images from files easily.\n",
    "\n",
    "To demonstrate the functionality of your network, in your final report include **Distances between pairs of pictures, picked from 2 pictures of yourself, and 2 pictures of other people (6 possible pairs in total). Also show all 4 pictures in your report.**\n",
    "\n",
    "(You can choose the pictures of other people freely. They can be headshots of the TAs on the course website or other famous people if you want.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd3d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the second image of the first person\n",
    "\n",
    "# 1. Read the image\n",
    "# 2. Detect and Crop\n",
    "# 3. Proprocess\n",
    "# 4. Run the model\n",
    "\n",
    "# process the image of the second person\n",
    "\n",
    "# Do the comparison of the distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af775b0",
   "metadata": {},
   "source": [
    "## Report Requirements & Grading (10 pts)\n",
    "You need to turn in a report and the code. You may turn in a separately-written report, but **in general, we recommend that you directly use the provided Jupyter Notebook and turn it in as your report.** For the first half of Part III, you don't need to implement all the code in the notebook, but you can run the load model step in the notebooks to demonstrate the result.\n",
    "\n",
    "### Part 1: (2 pts)\n",
    "* Show the command/code and output for getting the following (0.5 points each):\n",
    "    1. The CPU spec\n",
    "    2. The network configuration\n",
    "    3. The TensorFlow checks (version, GPU, etc.)\n",
    "* You can make screenshots of both the command/code and the resulting output or paste them directly into the report or the notebook.\n",
    "  \n",
    "### Part 2: (4 pts)\n",
    "* For each of the steps, briefly explain your approach and include the key outputs.\n",
    "* Show the training and testing accuracy of your model. For full credit, the testing accuracy should be over 85% and training accuracy should be over 90%. (2 Points)\n",
    "* Show the plots of train and validation loss and accuracy, similar to the ones shown before. (1 Point)\n",
    "* Show the accuracy and inference time of the TFLite models quantized with the two methods with and without the USB accelerator. Compare and comment on what you observed in the report. There might be no significant improvements in terms of performance, but you should try to understand and explain why in your report. (1 Point)\n",
    "  \n",
    "### Part 3: (4 pts)\n",
    "* For each of the steps, briefly explain your approach and include the key outputs.\n",
    "* Show the output of loading weights for your model to confirm that your model is correctly implemented. You can get partial credit if you cannot load the model correctly. (2 Points)\n",
    "* Show the bounding box of the face detected by MTCNN in the captured image, as described in the 'Face Detection' section of Part III. (1 Point)\n",
    "* Report the distances between pairs of pictures, picked from 4 pictures: 2 of yourself and 2 of other people (6 pairs in total). Also, show all 4 pictures in your report. (1 Point)\n",
    "\n",
    "### Notes: \n",
    "* Besides the above requirements, your report should also include the following details: \n",
    "  * Your full name and your NetID.\n",
    "  * The difficulties/bugs you encountered and how you solved them\n",
    "  * What you learned from this lab\n",
    "* Your report should cover all the required information, but please keep your report clean and concise.\n",
    "* Please add references to your report if you referred to any resources when you work on your lab.\n",
    "* Please submit your **report** to Canvas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd676ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
