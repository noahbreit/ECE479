{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ECE479 LAB 1: Python, NumPy and Data Analytics Basics\n",
    "====================================\n",
    "\n",
    "**Report Due: Feb 13, 11:59 PM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LAB 1, you will learn the basics of Python programming and the NumPy library by practicing basic image processing operations, including image cropping, Gaussian Filtering, and up/downsampling. In addition, you will have to implement two classical data analytics algorithms with Python and NumPy.\n",
    "\n",
    "If you do not have Python programming experience before, you can use the below links for a quick tutorial.\n",
    "\n",
    "Python tutorial: https://docs.python.org/3.9/tutorial/\n",
    "\n",
    "Python: How to fetch internet resources using urllib:\n",
    "https://docs.python.org/3/howto/urllib2.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART I: Python Basics: NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy is a foundational library in python that works with numbers and arrays. We will be using NumPy extensively to work with array computations in the following labs. Before we start, please check if you have successfully installed the Numpy package. NumPy [Tutorial][link_numpy_start] is available for your reference. (All blue words have hyperlinks embedded for your convenience.)\n",
    "\n",
    "[link_numpy_start]: https://docs.scipy.org/doc/numpy/user/quickstart.html\n",
    "\n",
    "To install dependencies for lab1, please use \"pip install -r requirements.txt\", or manually install all packages listed in the \"requirement.txt\" file.\n",
    "\n",
    "Once you are ready, please complete the following tasks: \n",
    "(NOTE: In the following tasks, you are **NOT** allowed to use explicit `for` loops. Each task should only require very few lines of code.)\n",
    "\n",
    "  * **Step 1**: Generate a 2-D all-zero NumPy array **A**, with the size of 9 x 6 (row x column). Print the type and shape of **A**.  \n",
    "  Hint: `type(A)`returns the type of an object `A`.\n",
    "  * **Step 2**: Create a block-I shape by replacing certain elements from 0 to 1 in array **A**.\n",
    "  * **Step 3**: Generate a 2-D array **B** by filling zero-vector at the top and bottom of the array **A**.\n",
    "  \n",
    "  ![sec1_img](figures/block-I.png)\n",
    "  \n",
    "  * **Step 4**: Generate a 2-D array **C**, with numbers 1, 2, 3, ..., 65, 66 filled in row by row, starting from the top-left corner of the array. The size of **C** is 11 x 6 (row x column, the same size as array **B**).\n",
    "  * **Step 5**: Perform element-wise multiplication between **B** and **C** and store the result in array **D**.\n",
    "  * **Step 6**: Take the non-zero elements in **D** and store into a new 1-D array **E**.\n",
    "  * **Step 7**: Normalize the elements in **E** and store in **F** using equations:  \n",
    "    ```Python\n",
    "        max, min = E.max(), E.min()\n",
    "\n",
    "    val_normalized = (val_original - min) / (max - min)\n",
    "  * **Step 8**: Reshape array F into a 2-D square matrix G. The size of G should be the square root of the number of elements in F, rounded down to the nearest integer. \n",
    "  * **Step 9**: Calculate the dot product of matrix G with its transpose (i.e., G * G<sup>T</sup>) and store the result in a new matrix H."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "\n",
      "(9, 6)\n"
     ]
    }
   ],
   "source": [
    "## Q1\n",
    "A = np.zeros(shape=(9,6))\n",
    "print(type(A))\n",
    "print()\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "## Q2\n",
    "# clear\n",
    "A[:] = 0\n",
    "\n",
    "# slice by row\n",
    "A[:2, 1:-1] = 1\n",
    "A[2:-2, 2:4] = 1\n",
    "A[-2:, 1:-1] = 1\n",
    "\n",
    "# print A\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Q3\n",
    "# make arr B with two more rows, then insert A vectors at all indexes excluding first and last\n",
    "B = np.zeros((A.shape[0] + 2, A.shape[1]))\n",
    "B[1:(1 + A.shape[0])] = A\n",
    "\n",
    "# print B\n",
    "print(B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 6)\n",
      "\n",
      "[[ 1.  2.  3.  4.  5.  6.]\n",
      " [ 7.  8.  9. 10. 11. 12.]\n",
      " [13. 14. 15. 16. 17. 18.]\n",
      " [19. 20. 21. 22. 23. 24.]\n",
      " [25. 26. 27. 28. 29. 30.]\n",
      " [31. 32. 33. 34. 35. 36.]\n",
      " [37. 38. 39. 40. 41. 42.]\n",
      " [43. 44. 45. 46. 47. 48.]\n",
      " [49. 50. 51. 52. 53. 54.]\n",
      " [55. 56. 57. 58. 59. 60.]\n",
      " [61. 62. 63. 64. 65. 66.]]\n"
     ]
    }
   ],
   "source": [
    "# Q4\n",
    "print(B.shape)\n",
    "print()\n",
    "C = np.zeros(B.shape)\n",
    "\n",
    "# parse C, fill C[index] = index + 1\n",
    "for i in range(0, C.shape[0]):\n",
    "    for j in range(0, C.shape[1]):\n",
    "        C[i][j] = (i * C.shape[1]) + (j + 1)\n",
    "\n",
    "# print\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  8.  9. 10. 11.  0.]\n",
      " [ 0. 14. 15. 16. 17.  0.]\n",
      " [ 0.  0. 21. 22.  0.  0.]\n",
      " [ 0.  0. 27. 28.  0.  0.]\n",
      " [ 0.  0. 33. 34.  0.  0.]\n",
      " [ 0.  0. 39. 40.  0.  0.]\n",
      " [ 0.  0. 45. 46.  0.  0.]\n",
      " [ 0. 50. 51. 52. 53.  0.]\n",
      " [ 0. 56. 57. 58. 59.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "## Q5\n",
    "# mult by element\n",
    "D = B * C\n",
    "\n",
    "# print\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.  9. 10. 11. 14. 15. 16. 17. 21. 22. 27. 28. 33. 34. 39. 40. 45. 46.\n",
      " 50. 51. 52. 53. 56. 57. 58. 59.]\n"
     ]
    }
   ],
   "source": [
    "## Q6\n",
    "# load array E with nonzero elements of array D\n",
    "E = D[D != 0]\n",
    "\n",
    "# print\n",
    "print(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          0.98039216  0.96078431  0.94117647  0.88235294  0.8627451\n",
      "  0.84313725  0.82352941  0.74509804  0.7254902   0.62745098  0.60784314\n",
      "  0.50980392  0.49019608  0.39215686  0.37254902  0.2745098   0.25490196\n",
      "  0.17647059  0.15686275  0.1372549   0.11764706  0.05882353  0.03921569\n",
      "  0.01960784 -0.        ]\n"
     ]
    }
   ],
   "source": [
    "## Q7\n",
    "# load array F with normalized elements of array E\n",
    "min, max = E.max(), E.min()\n",
    "F = np.zeros(E.shape)\n",
    "\n",
    "F = (E - min) / (max - min)\n",
    "\n",
    "# print\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.98039216 0.96078431 0.94117647 0.88235294]\n",
      " [0.8627451  0.84313725 0.82352941 0.74509804 0.7254902 ]\n",
      " [0.62745098 0.60784314 0.50980392 0.49019608 0.39215686]\n",
      " [0.37254902 0.2745098  0.25490196 0.17647059 0.15686275]\n",
      " [0.1372549  0.11764706 0.05882353 0.03921569 0.01960784]]\n"
     ]
    }
   ],
   "source": [
    "## Q8\n",
    "# load 2-D array G with shape(floor(sqrt(F.shape)))\n",
    "# with elements F\n",
    "import math\n",
    "\n",
    "row_g = math.floor(math.sqrt(F.shape[0]))\n",
    "G = np.zeros((row_g, row_g))\n",
    "\n",
    "for i in range(0, G.shape[0]):\n",
    "    for j in range(0, G.shape[1]):\n",
    "        G[i][j] = F[i * G.shape[0] + j]\n",
    "\n",
    "# print\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.54863514 3.82199154 2.52056901 1.19108035 0.3633218 ]\n",
      " [3.82199154 3.21491734 2.12341407 1.00807382 0.30949635]\n",
      " [2.52056901 2.12341407 1.41714725 0.67858516 0.21453287]\n",
      " [1.19108035 1.00807382 0.67858516 0.3348712  0.10841984]\n",
      " [0.3633218  0.30949635 0.21453287 0.10841984 0.03806228]]\n"
     ]
    }
   ],
   "source": [
    "## Q9\n",
    "# load arary H with dot product of G and G_transpose\n",
    "H = np.dot(G, G.T)\n",
    "\n",
    "# print\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: More Python: Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the basic knowledge of NumPy, we will move on to play with some images in the form of NumPy arrays. You may find these two packages helpful in this part: [pillow][pillow_doc] and [matplotlib][matplotlib_doc]. Pillow is an image library for python. It is capable of handling many image-processing related tasks. Matplotlib is a commonly used package for plotting graphs and images in python. We should only be using some very basic functions of those two libraries in the following tasks. \n",
    "\n",
    "**In your report, you should show the resulting images of each step.**\n",
    "\n",
    "  * **Step 1**: Read the image from `'figures/tulips.png'`, and convert it into a NumPy array. Print the type and the shape of the array. Display the array (which has been converted from the image). You can use [Image.open(...)][Image_open_doc] function from the pillow package to read the image and use [imshow(...)][imshow_doc] from the matplotlib package to display the cropped image (refer to **Step 2**). Notice that each pixel is represented by an integer number ranging from 0 to 255 in the output of `Image.open(...)`, so you may need to normalize pixel values to range [0.0, 1.0] (floating points) to make sure all pixels' values are still within the valid range after we apply the filters in following steps. Be careful, the image has three channels, which represent Red, Green and Blue respectively.\n",
    "\n",
    "[pillow_doc]:https://pillow.readthedocs.io/en/stable/\n",
    "[matplotlib_doc]:https://matplotlib.org/index.html\n",
    "[Image_open_doc]:https://pillow.readthedocs.io/en/stable/reference/Image.html\n",
    "[imshow_doc]:https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.imshow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 54 101  83]\n",
      "[0.21176471 0.39607843 0.3254902 ]\n"
     ]
    }
   ],
   "source": [
    "### import the packages and functions here ###\n",
    "### your code goes here ###\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# grab image, load np_array img_arr\n",
    "with Image.open('figures/tulips.png') as img:\n",
    "    #img.show()\n",
    "    img_arr = np.array(img.getdata())\n",
    "#print(img_arr)\n",
    "\n",
    "# nomalize img_arr values\n",
    "min, max = 0, 255\n",
    "img_arr_norm = np.zeros(img_arr.shape)\n",
    "\n",
    "img_arr_norm = (img_arr - min) / (max - min)\n",
    "\n",
    "# print\n",
    "print(img_arr[0])\n",
    "print(img_arr_norm[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Step 2**: Center-crop the image into dimension (200, 200, 3), meaning that you should crop the central portion of the image to the target height and width. For example:\n",
    "  ![center_crop_img](figures/center-crop.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import the packages and functions here ###\n",
    "### your code goes here ###\n",
    "cropped_width = 200\n",
    "cropped_height = cropped_width\n",
    "\n",
    "cropped_left = (img.width / 2) - (cropped_width / 2)\n",
    "cropped_top = (img.height / 2) - (cropped_height / 2)\n",
    "cropped_right = cropped_left + cropped_width\n",
    "cropped_bottom = cropped_top + cropped_height\n",
    "\n",
    "cropped_list = [cropped_left, cropped_top, cropped_right, cropped_bottom]\n",
    "img_cropped = img.crop(cropped_list)\n",
    "\n",
    "# print\n",
    "img_cropped.show()\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Step 3**: We will then apply filters to the processed image. In order to preserve the orginial dimensions (H, W, C) of the image after the filtering, we need to properly pad the image first. Similar to **PART I Step 3**, pad the image with zeros on all four edges. You can use [pad(...)][pad_doc] function from the numpy pacakge to do this.\n",
    "\n",
    "[pad_doc]:https://numpy.org/doc/stable/reference/generated/numpy.pad.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import the packages and functions here ###\n",
    "### your code goes here ###\n",
    "import math\n",
    "\n",
    "img_padded = Image.new(img.mode, (img.width, img.height), (0, 0, 0))\n",
    "img_padded.paste(img_cropped, (math.floor(cropped_left), math.floor(cropped_top)))\n",
    "\n",
    "img_padded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Step 4**: 2-D convolution is one of the very basic of operations of image processing. The 2-D convolution of a filter on an image is the operation of multiplying each pixel in the overlapping portion of the \"sliding window\" and then add these pixels together to make one output pixel. There is an excellent video explainning the basics [here][2d_conv_video]. You may read more [here][2d_conv]. \n",
    "    The pseudocode is shown below:\n",
    "    ```\n",
    "    for row in # of rows of output:\n",
    "        for col in # of columns of output:\n",
    "            for ch in # of channels of input/output:\n",
    "                sum = 0\n",
    "                for kh in # rows of filter:\n",
    "                    for kw in # columns of filter:\n",
    "                        sum += input[row+kh][col+kw][ch] * filter[kh][kw]\n",
    "                output[row][col][ch] = sum\n",
    "    ```\n",
    "    \n",
    "    It it important to note that this pseudocode is slightly different from what a convolution looks like in a Convolutional Neural Network (CNN). The difference comes from how we apply the filters. We will talk more about the convolutions in CNN later in the course. \n",
    "    \n",
    "    Now, use the following filters to process the original picture and display the results. **You should write your own 2-D convolution kernel code derived from the above pseudocode instead of using existing pre-written conv functions from any packages.**\n",
    "    \n",
    "    As a side note, the original image has three channels, and you should apply the filter to each of the three channels separately.\n",
    " \n",
    "    1. Gaussian Blur\n",
    "    ``` Python\n",
    "gaussian_filter = np.asarray([\n",
    "    [1,  4,  6,  4,  1],\n",
    "    [4, 16, 24, 16,  4],\n",
    "    [6, 24, 36, 24,  6],\n",
    "    [4, 16, 24, 16,  4],\n",
    "    [1,  4,  6,  4,  1]\n",
    "], dtype=np.float32)\n",
    "gaussian_blur = gaussian_filter/np.sum(gaussian_filter)\n",
    "    ```\n",
    "    2. Motion Blur\n",
    "    ``` Python\n",
    "motion_filter = np.asarray([\n",
    "    [1, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1],\n",
    "], dtype=np.float32)\n",
    "motion_blur = motion_filter/np.sum(motion_filter)\n",
    "    ```\n",
    "    3. Edge Detection\n",
    "    ``` Python\n",
    "edge_filter = np.asarray([\n",
    "  [-1, -1, -1],\n",
    "  [-1,  8, -1],\n",
    "  [-1, -1, -1]\n",
    "], dtype=np.float32)\n",
    "    ```\n",
    "[2d_conv_video]:https://www.youtube.com/watch?v=C_zFhWdM4ic\n",
    "[2d_conv]:https://www.allaboutcircuits.com/technical-articles/two-dimensional-convolution-in-image-processing/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import the packages and functions here ###\n",
    "### your code goes here ###\n",
    "### Part 1 Define a convolution function\n",
    "def conv_2d(input, filter):\n",
    "    num_kh, num_kw = filter.shape\n",
    "    num_row, num_col, num_ch = input.shape\n",
    "\n",
    "    output = np.zeros((input.shape), dtype=input.dtype)\n",
    "\n",
    "    print(input.shape)\n",
    "    print(output.shape)\n",
    "    print(filter.shape)\n",
    "\n",
    "    for row in range(0, num_row - num_kh):\n",
    "        for col in range(0, num_col - num_kw):\n",
    "            for ch in range(0, num_ch):\n",
    "                sum = 0\n",
    "                for kh in range(0, num_kh):\n",
    "                    for kw in range(0, num_kw):\n",
    "                        # if (row + kh >= num_row ):\n",
    "                        #     break\n",
    "                        # if (col + kw >= num_col ):\n",
    "                        #     break\n",
    "                        sum = sum + (input[row + kh][col + kw][ch] * filter[kh][kw])\n",
    "                output[row][col][ch] = sum\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 768, 3)\n",
      "(512, 768, 3)\n",
      "(5, 5)\n",
      "119600\n",
      "25\n",
      "124830\n"
     ]
    }
   ],
   "source": [
    "### Part 2 Apply the Gaussian filter\n",
    "gaussian_filter = np.asarray([\n",
    "  [1,  4,  6,  4,  1],\n",
    "  [4, 16, 24, 16,  4],\n",
    "  [6, 24, 36, 24,  6],\n",
    "  [4, 16, 24, 16,  4],\n",
    "  [1,  4,  6,  4,  1],\n",
    "], dtype=np.float32)\n",
    "gaussian_blur = gaussian_filter/np.sum(gaussian_filter)\n",
    "### import the packages and functions here ###\n",
    "### your code goes here ###\n",
    "img_padded_arr = np.asarray(img_padded)\n",
    "\n",
    "gaussian_blur_output_arr = conv_2d(img_padded_arr, gaussian_blur)\n",
    "\n",
    "#print(gaussian_blur_output_arr)\n",
    "print(np.count_nonzero(img_padded_arr))\n",
    "print(np.count_nonzero(gaussian_blur))\n",
    "print(np.count_nonzero(gaussian_blur_output_arr))\n",
    "\n",
    "img_gaussian_blur = Image.fromarray(gaussian_blur_output_arr)\n",
    "img_gaussian_blur.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 768, 3)\n",
      "(512, 768, 3)\n",
      "(7, 7)\n"
     ]
    }
   ],
   "source": [
    "### Part 3 Apply the Motion Blur filter\n",
    "motion_filter = np.asarray([\n",
    "    [1, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1],\n",
    "], dtype=np.float32)\n",
    "motion_blur = motion_filter/np.sum(motion_filter)\n",
    "### import the packages and functions here ###\n",
    "### your code goes here ###\n",
    "img_gaussian_blur_arr = np.asarray(img_gaussian_blur)\n",
    "\n",
    "motion_blur_output_arr = conv_2d(img_gaussian_blur_arr, motion_blur)\n",
    "\n",
    "img_motion_blur = Image.fromarray(motion_blur_output_arr)\n",
    "img_motion_blur.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 768, 3)\n",
      "(512, 768, 3)\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "### Part 4 Apply the Edge Detection filter\n",
    "edge_filter = np.asarray([\n",
    "  [-1, -1, -1],\n",
    "  [-1,  8, -1],\n",
    "  [-1, -1, -1]\n",
    "], dtype=np.float32)\n",
    "### import the packages and functions here ###\n",
    "### your code goes here ###\n",
    "img_motion_blur_arr = np.asarray(img_motion_blur)\n",
    "\n",
    "edge_filter_output_arr = conv_2d(img_motion_blur_arr, edge_filter)\n",
    "\n",
    "img_edge_filter = Image.fromarray(edge_filter_output_arr)\n",
    "img_edge_filter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * **Step 5**: Now you should pick filter(s) to apply advanced image processing techniques to enhance specific features of an image. This technique is often used to enhance details and sharpen images.\n",
    "    1. First, apply a blur to the original image $X$. Store this as Blurred Image.\n",
    "    2. Subtract the Blurred Image from the original image to get a Detail Enhancement image ($\\Delta = X - F(X)$).\n",
    "    3. Add the Detail Enhancement image back to the original image to produce a Sharpened Image ($\\Delta + X$).\n",
    "    4. Display the Blurred Image, Detail Enhancement image, and the final Sharpened Image.\n",
    "    \n",
    "    By displaying each picture in the process (three images in total), you may observe the effect of sharpening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 768, 3)\n",
      "(512, 768, 3)\n",
      "(7, 7)\n"
     ]
    }
   ],
   "source": [
    "### import the packages and functions here ###\n",
    "### your code goes here ###\n",
    "### Step 5: Image Sharpening\n",
    "img_x = img\n",
    "img_x_arr = np.asarray(img_x)\n",
    "blurred_arr = conv_2d(img_x_arr, motion_blur)\n",
    "img_blurred = Image.fromarray(blurred_arr)\n",
    "\n",
    "# img_x_arr_output is 'Blurred Image'\n",
    "# subtracted 'Blurred Image' from original to get delta\n",
    "delta_arr = img_x_arr - blurred_arr\n",
    "img_delta = Image.fromarray(delta_arr)\n",
    "\n",
    "# add delta to original to get sharp image\n",
    "sharp_arr = delta_arr + img_x_arr\n",
    "\n",
    "# convert sharp_arr to image\n",
    "img_sharp = Image.fromarray(sharp_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print\n",
    "#img_x.show()\n",
    "img_blurred.show()\n",
    "img_delta.show()\n",
    "img_sharp.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * **Step 6**: Also, we often need to downscale an image. We will use a basic algorithm to accomplish this: Gaussian pyramid. You can read about this technique [here][pyramid]. In this step, you will downsample the given image twice, each with a factor of two. In other words, you will first shrink the image to half of the size (2X smaller), then shrink it again by 2X. As a result, you will have an image that is one-fourth of its original size (4X smaller). We will try two approaches and see the difference.\n",
    "    1. Naive downsampling: you simply pick the 2nd, 4th, 6th, ..., nth pixels (every two pixels) by skipping one pixel each time. The resulting image should have one half of the size of the original. You repeat this process to obtain the 1/4 size image.\n",
    "    2. Gaussian pyramid: You apply the Gaussian filter to the image first, like what you have done in the previous step. After that, you downsample on the filtered image. Again, you pick the 2nd, 4th, 6th, ..., nth pixels by skipping one pixel each time when you downsample. Then, you repeat this process of \"filter and downsample\" to obtain the 1/4 size image. The Gaussian filter removes the high-frequency components from the image before downsampling, thus avoiding the aliasing problem as you will notice in the results.\n",
    "    \n",
    "    By displaying each picture in the process (two downsampled images per approach, four images in total), you may observe some difference in the results of the two techniques.\n",
    "[pyramid]:https://en.wikipedia.org/wiki/Pyramid_(image_processing)#:~:text=pyramid%20generation%20steps.-,Gaussian%20pyramid,lower%20level%20of%20the%20pyramid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import the packages and functions here ###\n",
    "### your code goes here ###\n",
    "### Part 1 Naive downsampling\n",
    "img_arr = np.asarray(img)\n",
    "downsize_arr = np.array( (math.floor(img_arr.shape[0] / 2), math.floor(img_arr.shape[1] / 2), img_arr.shape[2]) )\n",
    "\n",
    "downsize_arr = img_arr[::2, ::2, :]\n",
    "\n",
    "downsize2_arr = np.array( (math.floor(downsize_arr.shape[0] / 2), math.floor(downsize_arr.shape[1] / 2), downsize_arr.shape[2]) )\n",
    "\n",
    "downsize2_arr = downsize_arr[::2, ::2, :]\n",
    "\n",
    "img_downsize = Image.fromarray(downsize2_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print\n",
    "img.show()\n",
    "img_downsize.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 768, 3)\n",
      "(512, 768, 3)\n",
      "(5, 5)\n",
      "(256, 384, 3)\n",
      "(256, 384, 3)\n",
      "(5, 5)\n"
     ]
    }
   ],
   "source": [
    "### import the packages and functions here ###\n",
    "### your code goes here ###\n",
    "### Part 2 Gaussian filter + downsampling\n",
    "img_y = img\n",
    "img_y_arr = np.asarray(img_y)\n",
    "gaussian_img_y = conv_2d(img_y_arr, gaussian_blur)\n",
    "\n",
    "downsize_arr = gaussian_img_y[::2, ::2, :]\n",
    "\n",
    "downsize_arr = conv_2d(downsize_arr, gaussian_blur)\n",
    "\n",
    "downsize2_arr = downsize_arr[::2, ::2, :]\n",
    "\n",
    "img_downsize = Image.fromarray(downsize2_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print\n",
    "img.show()\n",
    "img_downsize.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART III: HTTP Communication, KNN and K-means in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the lab, we will implement a system that communicates with a server to acquire test data and perform [k-nearest neighbor][knn] (KNN) classification and K-means clustering on your computer.\n",
    "\n",
    "The overall operation of the classification system is depicted in the following figure:\n",
    "\n",
    "<!-- ![server_img](figures/server_behavior.svg) -->\n",
    "<img src=\"figures/server_behavior.svg\" width=\"60%\">\n",
    "\n",
    "The system works in a [RESTful][RESTful_tutorial]-like manner, and there are five main steps in the whole process:\n",
    "1. **Request for data**\n",
    "   The user sends an HTTP request with a specific body content to request a set of testing samples to be classified.\n",
    "2. **Receive data**\n",
    "   Upon receiving the request, the server will generate five random test IDs and select these samples from the testing dataset. Then, it sends back the test samples as a text string in comma-separated values (CSV) format in the response, and each sample will have a unique test ID and four separate features to be used for inference. For this part of the lab, we will use the classic [Iris dataset][iris_wiki]. The dataset is already randomly split into training and testing datasets. The training dataset will be provided to you, and the testing dataset will be stored on the server.\n",
    "3. **Local inference**\n",
    "   The local system parses the CSV file and performs the inference on the five input samples with kNN, which will be trained on the training dataset provided to you.\n",
    "4. **Send results**\n",
    "   Once the classes of the five samples are determined, create a new `POST` request with the test ID and the results and send them to the server for verification.\n",
    "5. **Return verification**\n",
    "   Upon receiving the request, the server will first use the test IDs to find the ground truth labels of the test samples and compare them with the ones from the `POST` request. Finally, the server sends back, in the response, a number indicating the number of correct classifications.\n",
    "\n",
    "As introduced in the lecture, KNN is a classic machine learning algorithm that does not assume pre-defined parameters or data distributions. K is the number of nearest neighbors. When k = 1, the algorithm is simply the nearest neighbor algorithm. Intuitively, in the nearest neighbor algorithm, the unknown data point is classified as the same label as the known data point from which it has the least distance in the hyperspace, as shown in the following picture. \n",
    "\n",
    "![knn_k1_img](figures/knn_k1.png)\n",
    "\n",
    "In the k-nearest neighbor algorithm, you pick k data points that are closest to the unknown data. The k known data points make a collective decision on which label the unknown data should be assigned. The following three pictures show the process. \n",
    "\n",
    "![knn_k_img](figures/knn_k.png)\n",
    "\n",
    "<sup>(picture source: https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn)</sup>\n",
    "\n",
    "On the other side, K-means clustering is one of the most popular unsupervised machine learning algorithms. The K-means algorithm in data mining starts with a K group of randomly selected centroids, which are used as the beginning points for every cluster, and then performs iterative (repetitive) calculations to optimize the positions of the centroids. The process is shown in the following picture.\n",
    "\n",
    "It halts creating and optimizing clusters when either:\n",
    "\n",
    "1. The centroids have stabilized — there is no change in their values because the clustering has been successful.\n",
    "2. The defined number of iterations has been achieved.\n",
    "\n",
    "<img src=\"figures/kmeans.png\" width=\"500\" height=\"500\">\n",
    "<!-- ![kmeans_img](figures/kmeans.png).size(0.5) -->\n",
    "\n",
    "<sup>(picture source: https://towardsdatascience.com/k-means-a-complete-introduction-1702af9cd8c)</sup>\n",
    "\n",
    "**Important**: You should only use the Python packages that are introduced in this lab: NumPy, Requests, and Pandas. You may use some plotting packages to visualize the datasets if you feel extra fancy. However, you should not use any machine learning packages such as scikit-learn.\n",
    "\n",
    "[RESTful_tutorial]:https://restfulapi.net/\n",
    "[knn]:https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
    "[iris]:https://courses.grainger.illinois.edu/ece479/sp2023/secure/labs/lab1/iris_train.csv\n",
    "[iris_label]:https://courses.grainger.illinois.edu/ece479/sp2023/secure/labs/lab1/iris_label.csv\n",
    "[iris_wiki]:https://en.wikipedia.org/wiki/Iris_flower_data_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Step 1**: Download the Iris dataset from [here][iris] and the labels from [here][iris_label]. Read the csv datasets into arrays. You may use the following code:\n",
    "\n",
    "    ``` Python\n",
    "import pandas as pd\n",
    "data = pd.read_csv('iris.csv', header=0, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
    "labels = pd.read_csv('iris_labels.csv', header=0, names=['label'])\n",
    "    ```\n",
    "[iris]:https://courses.grainger.illinois.edu/ece479/sp2024/secure/labs/lab1/iris_train.csv\n",
    "[iris_label]:https://courses.grainger.illinois.edu/ece479/sp2024/secure/labs/lab1/iris_train_label.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal_length  sepal_width  petal_length  petal_width\n",
      "0             5.1          3.5           1.4          0.2\n",
      "1             4.9          3.0           1.4          0.2\n",
      "2             4.7          3.2           1.3          0.2\n",
      "3             4.6          3.1           1.5          0.2\n",
      "4             5.0          3.6           1.4          0.2\n",
      "..            ...          ...           ...          ...\n",
      "145           6.7          3.0           5.2          2.3\n",
      "146           6.3          2.5           5.0          1.9\n",
      "147           6.5          3.0           5.2          2.0\n",
      "148           6.2          3.4           5.4          2.3\n",
      "149           5.9          3.0           5.1          1.8\n",
      "\n",
      "[150 rows x 4 columns]\n",
      "         label\n",
      "0       setosa\n",
      "1       setosa\n",
      "2       setosa\n",
      "3       setosa\n",
      "4       setosa\n",
      "..         ...\n",
      "145  virginica\n",
      "146  virginica\n",
      "147  virginica\n",
      "148  virginica\n",
      "149  virginica\n",
      "\n",
      "[150 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "### import the packages and functions here ###\n",
    "### your code goes here ###\n",
    "import pandas as pd\n",
    "data = pd.read_csv('data/iris.csv', header=0, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
    "labels = pd.read_csv('data/iris_label.csv', header=0, names=['label'])\n",
    "\n",
    "print(data)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Step 2**: Make a `POST` request with the following specifications:\n",
    "\n",
    "    ``` Python\n",
    "url = 'http://courses.grainger.illinois.edu/ece479/sp2024/lab1_request_dataset.php'\n",
    "body = {'request': 'testdata'}\n",
    "    ```\n",
    "    Please read the first three sections of [guide](https://requests.readthedocs.io/en/master/user/quickstart/) to learn how to send HTTP requests and parse responses from the server.\n",
    "    Also, make sure that you make request to `https` (SSL certificated) instead of `http`. If your request is properly made, the server should return a content in comma-separated values (CSV) format. Parse the content into an array, and this array will be used as your testing dataset in the following steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.   5.7  3.8  1.7  0.3]\n",
      " [ 3.   6.   2.9  4.5  1.5]\n",
      " [ 8.   6.2  2.2  4.5  1.5]\n",
      " [11.   4.8  3.   1.4  0.1]\n",
      " [14.   5.1  3.8  1.5  0.3]]\n"
     ]
    }
   ],
   "source": [
    "### import the packages and functions here ###\n",
    "### your code goes here ###\n",
    "import requests\n",
    "import re\n",
    "\n",
    "url = 'https://courses.grainger.illinois.edu/ece479/sp2024/lab1_request_dataset.php'\n",
    "body = {'request': 'testdata'}\n",
    "r = requests.post(url, body)\n",
    "\n",
    "r_temp = re.split(',|\\n', r.text)\n",
    "testdata_arr = np.zeros((5, 5))\n",
    "\n",
    "for i in range(0, 5):\n",
    "    for j in range(0, 5):\n",
    "        testdata_arr[i][j] = r_temp[(i + 1) * 5 + j]\n",
    "print(testdata_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Step 3**: We will use the Euclidean distance to find the nearest neighbors. Write a function that calculates the distance between two data points. Note: each data point has four features, which means each $X$ is a four-dimensional variable. Recall that the Euclidean distance is defined as $\\sqrt{\\sum_{i=1}^{N=4}(x^{(i)}_1-x^{(i)}_2)^2}$. \n",
    "  \n",
    "  **Hint**: this can be easily done as a one-liner with Numpy, try to avoid using nested loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import the packages and functions here ###\n",
    "### your code goes here ###\n",
    "import math\n",
    "\n",
    "# B input in pd.dataframe iterrow object\n",
    "def dist(A, B):\n",
    "    sum = 0\n",
    "    for i in range(1, 5):\n",
    "        sum += ((A[i] - B[1][i - 1])**2)\n",
    "    return math.sqrt(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Step 4**: Implement a function that predicts the classes of the five testing samples using kNN. To perform kNN for a testing sample, first calculate the Euclidean distances between the testing sample and all the samples in the training dataset that are given to you. Then, rank the training samples by their distances to the testing sample, and find the k points that have the least distances to the testing sample. Finally, take the most common labels of the k nearest neighbors and use that to classify the unknown testing data point. \n",
    "    You may experiment with different values of k to see if they give you different answers. We recommend that you try k = 3, 5, or 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import the packages and functions here ###\n",
    "### your code goes here ###\n",
    "def classify_k(test_set):\n",
    "     k = 5\n",
    "     prediction = []\n",
    "\n",
    "     for test_point in test_set:\n",
    "          # fetch curr train point data\n",
    "          distance = np.asarray([dist(test_point, train_point) for train_point in data.iterrows()])\n",
    "\n",
    "          # sort distances, get k nearest neighbors\n",
    "          nearest_idx = np.argsort(distance)[:k]\\\n",
    "\n",
    "          # make list of corresponding labels\n",
    "          nearest_label = []\n",
    "          for idx in nearest_idx:\n",
    "               label = labels.loc[idx]['label']\n",
    "               nearest_label.append(label)\n",
    "\n",
    "          flower_a, flower_b, flower_c = 0, 0, 0\n",
    "          for label in nearest_label:\n",
    "               if (label == \"versicolor\"):\n",
    "                    flower_a += 1\n",
    "               elif (label == \"setosa\"):\n",
    "                    flower_b += 1\n",
    "               elif (label == \"virginica\"):\n",
    "                    flower_c += 1\n",
    "               else:\n",
    "                    print(\"bad match\")\n",
    "                    return\n",
    "          nearest_label.clear()\n",
    "                    \n",
    "          if (flower_a > flower_b and flower_a > flower_c):\n",
    "               prediction.append(\"versicolor\")\n",
    "          elif (flower_b > flower_a and flower_b > flower_c):\n",
    "               prediction.append(\"setosa\")\n",
    "          elif (flower_c > flower_a and flower_c > flower_b):\n",
    "               prediction.append(\"virginica\")\n",
    "\n",
    "     # return predictions\n",
    "     # format for json\n",
    "     str = ','.join(prediction)\n",
    "     prediction.clear()\n",
    "     return str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Step 5**: Make a `POST` request again to the same URL. This time, the `request` field will be `verify`, and you should specify the two additional `test_ids` and the `predictions` fields. They should be in the form of ',' deliminated list, i.e. \\\"1,3,5,6,7\\\" for the `test_ids`. The structure of your request should be like the following:\n",
    "```python\n",
    "    {'request':'verify',\n",
    "     'test_ids':'1,3,6,8,12',\n",
    "     'predictions':'setosa,setosa,versicolour,virginica,setosa'}\n",
    "```\n",
    "The server will return the number of correct predictions on the testing samples drawn from the testing dataset. The accuracy should be 100% in most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_id,sepal_length,sepal_width,petal_length,petal_width\n",
      "1,5.7,3.8,1.7,0.3\n",
      "5,5.4,3.4,1.5,0.4\n",
      "7,6.9,3.1,5.1,2.3\n",
      "10,6.5,3.2,5.1,2\n",
      "12,5.5,3.5,1.3,0.2\n",
      "\n",
      "1,5,7,10,12\n",
      "setosa,setosa,virginica,virginica,setosa\n",
      "\n",
      "5  out of  5\n"
     ]
    }
   ],
   "source": [
    "### import the packages and functions here ###\n",
    "### your code goes here ###\n",
    "url = 'https://courses.grainger.illinois.edu/ece479/sp2024/lab1_request_dataset.php'\n",
    "body = {'request': 'testdata'}\n",
    "\n",
    "r = requests.post(url, body)\n",
    "\n",
    "r_temp = re.split(',|\\n', r.text)\n",
    "\n",
    "print(r.text)\n",
    "print()\n",
    "\n",
    "for i in range(0, 5):\n",
    "    for j in range(0, 5):\n",
    "        testdata_arr[i][j] = r_temp[(i + 1) * 5 + j]\n",
    "\n",
    "testdata_ids = ','.join(r_temp[5::5])\n",
    "testdata_prediction = classify_k(testdata_arr)\n",
    "\n",
    "print(testdata_ids)\n",
    "print(testdata_prediction)\n",
    "\n",
    "### VERIFY ###\n",
    "body = {'request': 'verify',\n",
    "        'test_ids': testdata_ids,\n",
    "        'predictions': testdata_prediction} \n",
    "\n",
    "r = requests.post(url, body)\n",
    "\n",
    "print()\n",
    "print(r.text, \" out of \", len(testdata_arr[:, 0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Step 6**: Implement a function that finds the k centroids of the iris dataset using K-means. To perform K-means for a dataset, first, initialize random k centroids. Then for each data point, find the nearest euclidean distance to centroids and assign the point to that cluster. In addition, for each cluster, compute the new centroid equals the mean of all points assigned to that cluster. You should repeat the distance calculation, and centroids update until convergence or until the end of a fixed number of iterations. We recommend that you try k = 2, 3, or 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_centroids(arr, centroids):\n",
    "    ##\n",
    "    k = 3\n",
    "    ##\n",
    "\n",
    "    idx = np.zeros(arr.shape[0], dtype = int)\n",
    "    for curr_point in arr.shape[0]:\n",
    "        distance = np.asarray([distance(curr_point, point) for point in centroids.iterrows()])\n",
    "        idx[i] = np.argsort(distance)[:k]\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import the packages and functions here ###\n",
    "### your code goes here ###\n",
    "def k_means(arr, init_centroids):\n",
    "    ##\n",
    "    num_loop = 10\n",
    "    k = 3\n",
    "    ##\n",
    "\n",
    "    # init\n",
    "    m, n = arr.shape\n",
    "    centroids = init_centroids\n",
    "    prev_centroids = centroids\n",
    "    idx = np.zeros(m)\n",
    "\n",
    "    ###\n",
    "    for i in range(num_loop):\n",
    "        idx = find_centroids(arr, centroids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Requirements: (10 pts)\n",
    "You have to do a live demo for this lab during your designated lab session. In addition, you will need to turn in a report and the code. **We highly recommend that you directly use the provided Jupyter Notebook and turn it in as a combination of the report and the code.**\n",
    "\n",
    "### Part 1: (2 pts)\n",
    "* Show the code and the results of each step. Discuss your approaches for correct generations of all NumPy arrays **A** to **G**, with each worth 0.25 points.\n",
    "* Since the code should be very short, you can either type it directly in the report or take screenshots of both the code and the resulting arrays and paste the screenshot into the report.\n",
    "  \n",
    "### Part 2: (3 pts)\n",
    "* Show the results of each step. Discuss your appoarches. There is no need to include the code in the report. \n",
    "* For step 1, show the type and the shape of the array, print the original image. For step 2, show the cropped image. Nothing needs to be shown for step 3. The whole step 1 to 3 worth 1 point. \n",
    "* For step 4, show the resulting images of the three filtering processes. Step 4 worths 1 point.\n",
    "* For step 5, show the three images for three steps. Step 5 worths 0.5 point.\n",
    "* For step 6, show the four images from the two different approaches. Step 6 worths 0.5 point.\n",
    "\n",
    "  \n",
    "### Part 3: (5 pts)\n",
    "* Show the results of each step. Discuss your appoarches. There is no need to include the code in the report. \n",
    "* For step 1, print the data you read from both csv files. For step 2, acquire at least 3 datasets from the server and print the response. You may take screenshots to show the prints. Step 1 and 2 worth 1 point.\n",
    "* For step 3 and 4, briefly discuss your code, including what k values that you have tried. Show a screenshot of the two functions if they can be contained in one page. The code shouldn't be very long. Step 3 and 4 worth 2 points.\n",
    "* For step 5, print the responses from the server. Step 5 worths 1 point.\n",
    "* For step 6, print the centroid values of k=3. Step 6 worths 1 point.\n",
    "\n",
    "\n",
    "### Notes: \n",
    "* Besides the above requirements, your report should also include the following details: \n",
    "  * Your full name and your NetID.\n",
    "  * The difficulties/bugs you encountered and how you solved them\n",
    "  * What you learned from this lab\n",
    "* Your report should cover all the required information, but please keep your report clean and concise.\n",
    "* Please add references to your report if you referred to any resources when you work on your lab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
